# LLMs Evolutionary Merging for L2S Math Reasoning

Project for **Deep Learning and Applied AI 2024/2025 (Sapienza University of Rome)**.  
We explore **evolutionary model merging** (via [Mergenetic](https://arxiv.org/abs/2505.11427)) we to combine:
- **DeepSeek-R1-Distill-Qwen** (slow-thinking, accurate Chain-of-Thought model),
- **Qwen2.5-Math** (fast-thinking, concise direct-answer model),

to improve the trade-off between **accuracy** and **response length** in **Long-to-Short (L2S) mathematical reasoning**.

---

## ğŸ“‚ Repository Structure

```
LLMs Evolutionary Merging for L2S Math Reasoning
|
â”œâ”€â”€ project_notebook.ipynb                   # step-by-step execution for merge and evaluation 
|
â”œâ”€â”€ images/                                  # Figures for report
â”‚
â”œâ”€â”€ Qwen2.5-Math/                            # Evaluation framework directory
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ math_eval.py                     # Core evaluation function
â”‚       â”œâ”€â”€ compute_NTR.py                   # Negative Transfer analysis
â”‚       â”œâ”€â”€ generate_score_tensors.py        # Generate binary tensor for each benchmatk: 1=correct, 0=wrong answer
â”‚       â”œâ”€â”€ extract_result.py                # Extract results from json/jsonl output files
â”‚       â”œâ”€â”€ sh/
â”‚       â”‚   â”œâ”€â”€ l2s_eval.sh                  # Generic evaluation script â†’ calls math_eval.py
â”‚       â”‚   â””â”€â”€ qwen_eval.sh                 # Wrapper (set params â†’ calls l2s_eval.sh)
â”‚       â”œâ”€â”€ data/                            # Benchmarks (gsm8k, aime24, math500, minerva, etc.)
â”‚       â””â”€â”€ outputs/                         # Results (Baselines, Mergenetic, etc.)
â”‚
â””â”€â”€ mergenetic/                              # Evolutionary merging framework directory
    â”œâ”€â”€ README.md
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ requirements_nb.txt
    â”œâ”€â”€ environment.yml
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ run_mergekit.py                  # MergeKit utility function
    â”‚   â”œâ”€â”€ mergenetic_gsm8k_TA.py           # end-to-end Task Arithmetic merge
    â”‚   â””â”€â”€ mergenetic_gsm8k_TIES.py         # end-to-end TIES merge
    â”œâ”€â”€ src/mergenetic/                      # Core library
    â”‚   â”œâ”€â”€ merging/
    â”‚   â”œâ”€â”€ optimization/
    â”‚   â”œâ”€â”€ evaluation/
    â”‚   â”œâ”€â”€ estimator/
    â”‚   â”œâ”€â”€ searcher/
    â”‚   â””â”€â”€ utils.py
    â”œâ”€â”€ models/                              # folder to store DeepSeek, Qwen, merged models
    â””â”€â”€ experiments/                         # Results of evolutionary runs (CSV logs, configs)
```

---

## ğŸ› ï¸ Environment Setup

There is **no single global requirements file**. Each component has its own environment:

- `mergenetic/requirements.txt` â†’ for merging  
- `Qwen2.5-Math/evaluation/requirements.txt` â†’ for evaluation  


```bash
# ğŸ Create and activate the virtual environment
python3.11 -m venv ~/mergenetic/.venv
source ~/mergenetic/.venv/bin/activate

# ğŸ“¦ Install all required dependencies 

## Install basic packages for notebooks:
pip install jupyter ipykernel
pip install --upgrade pip

## dependencies for Mergenetic framework
cd mergenetic
pip install -r requirements.txt
pip install -e .

## dependencies for Qwen2.5-Math evaluation framework
cd ../Qwen2.5-Math/evaluation
pip install -r requirements.txt
cd latex2sympy  
pip install -e . 
```

---

## ğŸš€ Usage

You can either run **scripts end-to-end** or follow the **notebooks** for step-by-step execution.

### 1. Merging (Task Arithmetic / TIES)

From `mergenetic/`:
**Task Arithmetic (TA)**
```bash
python mergenetic/scripts/mergenetic_gsm8k_TA.py 
```
**TIES**
```bash
python mergenetic/scripts/mergenetic_gsm8k_TIES.py
```
### 2. Evaluation

Two options:

**A) Shell scripts**  
- Edit `evaluation/sh/qwen_eval.sh` to set:
  - `MODEL_PATH` â†’ path to the model checkpoint
  - `OUTPUT_DIR` â†’ folder to store results
  - `DATASETS`, `SEED`, etc. as needed  
- Run:
  ```bash
  bash evaluation/sh/qwen_eval.sh
  ```
  Execution flow:  
  `qwen_eval.sh` â†’ calls `l2s_eval.sh` â†’ calls `math_eval.py`.

**B) Notebook**  
- Follow the Evaluation section of the notebook and run cells that call `math_eval.py` directly.  
- This is the easiest option if you want a **guided, step-by-step run**.

Results are stored under `Qwen2.5-Math/evaluation/outputs/.`.

---

## ğŸ“Š Metrics & Benchmarks

- **Accuracy**: Exact Match (on numerical answers)  
- **Output length**: average tokens generated  
- **Negative Transfer Rate (NTR)** (optional)  
- Benchmarks: `gsm8k`, `aime24`, `math`, `minerva_math`, `olympiadbench`, `college_math`

---

## ğŸ‘¤ Author

Mario Iacobelli â€” 1841427  
Sapienza University of Rome
